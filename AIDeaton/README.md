# AIchemist AIDeaton â¤ï¸â€ğŸ©¹ì‹¬ì¿µíŒ€

[Heart Attack Analysis & Prediction Dataset](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/code?datasetId=1226038&sortBy=voteCount)

# âœ… ë°ì´í„° ì‚´í´ë³´ê¸°

## **About this dataset**

- age : Age of the patient
- sex : Sex of the patient
- cp : Chest Pain type
    - Value 0: typical angina
    - Value 1: atypical angina
    - Value 2: non-anginal pain
    - Value 3: asymptomatic
- trtbps : resting blood pressure (in mm Hg)
- chol : cholestoral in mg/dl fetched via BMI sensor
- fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
- rest_ecg : resting electrocardiographic results
    - Value 0: normal
    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
- thalachh : maximum heart rate achieved
- exng: exercise induced angina (1 = yes; 0 = no)
- caa: number of major vessels (0-3)
- target : 0 = less chance of heart attack, 1 = more chance of heart attack

---

```python
heart_data = pd.read_csv('heart.csv')
heart_data.head()
```
```python
heart_data.info()
```
### â†’ NULLê°’ì€ ì—†ëŠ” ê²ƒìœ¼ë¡œ íŒŒì•…ëœë‹¤!

```python
heart_data.describe( )
```

# âœ… ë°ì´í„° ì „ì²˜ë¦¬

## ì‹œê°í™”

### **íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì—°ì†í˜•/ë²”ì£¼í˜• ë³€ìˆ˜ì¸ì§€ í™•ì¸**

```python
plt.figure(figsize=(15,15))
for i,col in enumerate(heart_data.columns,1):
    plt.subplot(7,2,i)
    plt.title(f"Distribution of {col} Data")
    sns.histplot(heart_data[col],kde=True)
    plt.tight_layout()
    plt.plot()
```


- ì—°ì†í˜•: **age, trtbps, chol, thalach, oldpeak**
- ë²”ì£¼í˜•: **sex, cp, fbs, restecg, exng, slp, caa**, thall, output

```python
cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']
con_cols = ["age","trtbps","chol","thalachh","oldpeak"]
```

### ì—°ì†í˜• ë³€ìˆ˜ boxplotìœ¼ë¡œ ì‹œê°í™”

```python
# í”Œë¡¯ì˜ í–‰ê³¼ ì—´ì„ ì„¤ì •
n_rows = 3
n_cols = 2

# matplotlibì˜ subplotsë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10))

# ê° ë³€ìˆ˜ì— ëŒ€í•œ Boxplotì„ ê·¸ë¦½ë‹ˆë‹¤.
for i, var in enumerate(con_cols):
    row = i // n_cols
    col = i % n_cols
    sns.boxplot(x='output', y=var, data=heart_data, ax=axes[row, col])
```


- cholì€ ì‹¬ì¥ë§ˆë¹„ í™˜ìêµ°ì˜ ìˆ˜ì¹˜ê°€ ì˜¤íˆë ¤ ë‚®ì•˜ìŒ
- thakachhì€ ìµœì†Ÿê°’, ì¤‘ì•™ê°’, ìµœëŒ“ê°’ ë“±ì´ ëª¨ë‘ ì‹¬ì¥ë§ˆë¹„ í™˜ìêµ°ì—ì„œ ë†’ê²Œ ë‚˜íƒ€ë‚¨

### **ë²”ì£¼í˜• ë³€ìˆ˜ barplotìœ¼ë¡œ ì‹œê°í™”**

```python
# í”Œë¡¯ì˜ í–‰ê³¼ ì—´ì„ ì„¤ì •
n_rows = 2
n_cols = 4

# matplotlibì˜ subplotsë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 10))

# ê° ë³€ìˆ˜ì— ëŒ€í•œ Barplotì„ ê·¸ë¦½ë‹ˆë‹¤.
for i, var in enumerate(cat_cols):
    row = i // n_cols
    col = i % n_cols
    sns.countplot(x='output', hue=var, data=heart_data, ax=axes[row, col])
```

- cp: type=0ì¸ ê²½ìš° ë°œë³‘í™•ë¥  ë‚®ìŒ, type=2ì¸ ê²½ìš° ë°œë³‘í™•ë¥  ë†’ìŒ
- rest_ecg
    - Value 0: normal â†’ ë°œë³‘í™•ë¥  ë‚®ìŒ
    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) â†’ ë°œë³‘í™•ë¥  ë†’ìŒ
    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
- exng: 0ì¸ ê²½ìš°(exercise induced anginaê°€ ì•„ë‹Œê²½ìš°) ë°œë³‘í™•ë¥  ë†’ìŒ
- caa: type=0ì¸ ê²½ìš° ë°œë³‘í™•ë¥  ë†’ìŒ

## ì´ìƒì¹˜ ì œê±°

- ìƒê´€ë„ í™•ì¸: `.corr()`

```python
corr = heart_data.corr()

# ìƒê´€ê´€ê³„ ì‹œê°í™”
# -1 ë˜ëŠ” 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒê´€ê´€ê³„ ë†’ì€ ê²ƒ
sns.clustermap(corr, 
               annot = True,      # ì‹¤ì œ ê°’ í™”ë©´ì— ë‚˜íƒ€ë‚´ê¸°
               cmap = 'RdYlBu_r',  # Red, Yellow, Blue ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œ
               vmin = -1, vmax = 1, # ì»¬ëŸ¬ì°¨íŠ¸ -1 ~ 1 ë²”ìœ„ë¡œ í‘œì‹œ
              )
```


- ì´ìƒì¹˜ ì œê±°: IQR ì´ìš©

```python
# 1ì‚¬ë¶„ìœ„ìˆ˜(Q1) ë° 3ì‚¬ë¶„ìœ„ìˆ˜(Q3) ê³„ì‚°
Q1 = heart_data.quantile(0.25)
Q3 = heart_data.quantile(0.75)

# IQR ê³„ì‚°
IQR = Q3 - Q1

# ì„ê³„ê°’ ì„¤ì • (ì¼ë°˜ì ìœ¼ë¡œ 1.5 ì´ìƒì„ ì‚¬ìš©)
threshold = 1.5

# ì´ìƒì¹˜ ì œê±°
clean_data = heart_data[~((heart_data < (Q1 - threshold * IQR)) | (heart_data > (Q3 + threshold * IQR))).any(axis=1)]
```


## ì¸ì½”ë”©

ì‚¬ì´í‚·ëŸ°ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ë¬¸ìì—´ ê°’ì— ëŒ€í•´ ì ìš©í•  ìˆ˜ ì—†ìŒ 

â†’ í”¼ì²˜ë“¤ì˜ ìë£Œí˜•ì„ í™•ì¸í–ˆì„ ë•Œ ë¬¸ì¥í˜•, ì¦‰ objectí˜• í”¼ì²˜ê°€ ìˆì„ ê²½ìš° ì´ë“¤ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•¨

- ë¨¸ì‹ ëŸ¬ë‹ì˜ ëŒ€í‘œì ì¸ ì¸ì½”ë”© ë°©ì‹
    - **ë ˆì´ë¸” ì¸ì½”ë”©:** ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ë¥¼ ì½”ë“œí˜• ìˆ«ì ê°’ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì‘ì—…
        
        â†’ ë¬¸ìí˜•ìœ¼ë¡œ ë˜ì–´ìˆëŠ” í”¼ì²˜ê°€ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ë¨
        
    - **ì›-í•« ì¸ì½”ë”©:** ë°ì´í„°ì˜ ìœ í˜•ì— ë”°ë¼ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ìœ  ê°’ì— í•´ë‹¹í•˜ëŠ” ì¹¼ëŸ¼ì—ë§Œ 1ì„ í‘œì‹œí•˜ê³  ë‚˜ë¨¸ì§€ ì¹¼ëŸ¼ì—ëŠ” 0ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ë°©ë²•
        
        â†’ ì—´ë“¤ì˜ ê³ ìœ  ê°’ì„ ì°¨ì›ì„ ë³€í™˜í•œ ë’¤, í•´ë‹¹í•˜ëŠ” ì¹¼ëŸ¼ì—ë§Œ 1ì„ í‘œì‹œ, ë‚˜ë¨¸ì§€ì—ëŠ” 0ì„ í‘œì‹œ(pandasì˜ `get_dummies` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ìˆ«ìí˜• ê°’ ë³€í™˜ì—†ì´ ì›-í•« ì¸ì½”ë”©ì„ ì‰½ê²Œ êµ¬í˜„ í•  ìˆ˜ ìˆìŒ)
        

### **â†’ ë²”ì£¼í˜• ë°ì´í„°ì¸ ê²½ìš° ì›-í•« ì¸ì½”ë”© ì ìš© (ì´ë¯¸ ë²”ì£¼í˜• ë°ì´í„°ê°€ ìˆ«ìí˜•ì„)**

## í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”, ì •ê·œí™”)

: ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ê°’ ë²”ìœ„ë¥¼ ì¼ì •í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ëŠ” ì‘ì—…

- **í‘œì¤€í™”:** í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸, â€˜ê°€ìš°ì‹œì•ˆ ì •ê·œ ë¶„í¬â€™ë¥¼ ê°€ì§„ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì‘ì—…
- **ì •ê·œí™”:** ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ ë°ì´í„°ë“¤ì´ ìˆì„ ë•Œ, ì´ë“¤ì˜ í¬ê¸°ë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ í¬ê¸°ë¥¼ ë³€í™˜í•´ì£¼ëŠ” ì‘ì—… â†’ ê°œë³„ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ëª¨ë‘ ë˜‘ê°™ì€ ë‹¨ìœ„ë¡œ ë³€ê²½
- **ë¡œë²„ìŠ¤íŠ¸(Robust)**:Â ë°ì´í„°ì˜ ì¤‘ì•™ê°’ = 0, IQR = 1ì´ ë˜ë„ë¡ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê¸°ë²•
    - `RobustScaler`ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë“  ë³€ìˆ˜ë“¤ì´ ê°™ì€ ìŠ¤ì¼€ì¼ì„ ê°–ê²Œ ë˜ë©°, `StandardScaler`ì— ë¹„í•´ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ê°€ ë” ë„“ì€ ë²”ìœ„ë¡œ ë¶„í¬
    - `StandardScaler`**ì— ë¹„í•´ ì´ìƒì¹˜ì˜ ì˜í–¥ì´ ì ì–´ì§„ë‹¤ëŠ” ì¥ì **
    - ê¸ˆìœµ ë°ì´í„° ë¶„ì„, **ìƒë¬¼í•™ì  ë°ì´í„° ì²˜ë¦¬**, **ì„¼ì„œ ë°ì´í„° ë¶„ì„** ë“±ì—ì„œ ì´ìƒì¹˜ê°€ ìì£¼ ë°œìƒí•˜ëŠ” ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©

### â†’ `RobustScaler` ì‚¬ìš©

```python
# creating a copy of df
df = clean_data

# encoding the categorical columns
df = pd.get_dummies(df, columns = cat_cols, drop_first = True)

# defining the features and target
X = df.drop(['output'],axis=1)
y = df[['output']]

# instantiating the scaler
scaler = RobustScaler()

# scaling the continuous featuree
X[con_cols] = scaler.fit_transform(X[con_cols])
print("The first 5 rows of X are")
X.head()
```


# âœ… ëª¨ë¸ë§

ì •ë‹µ ë ˆì´ë¸”ì¸ â€˜ì‹¬ì¥ë§ˆë¹„ ë°œë³‘ ì—¬ë¶€â€™ ë°ì´í„°ë¥¼ ì•Œê³  ìˆìŒ

**â†’ ì§€ë„ í•™ìŠµ**

ì§€ë„í•™ìŠµì€ íšŒê·€ë¬¸ì œì™€ ë¶„ë¥˜ë¬¸ì œë¡œ ë‚˜ë‰¨

<aside>
ğŸ’¡ **íšŒê·€ vs ë¶„ë¥˜**

- íšŒê·€ì™€ ë¶„ë¥˜ì˜ ê³µí†µì 
    - ì§€ë„í•™ìŠµ: ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¥¼ í™œìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•
- íšŒê·€ì™€ ë¶„ë¥˜ì˜ ì°¨ì´ì 
    
    
    | íšŒê·€ ( Regression ) | ë¶„ë¥˜ ( Classification) |
    | --- | --- |
    | ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿê°’ì´ ì‹¤ìˆ˜(ìˆ«ì)ì¸ ê²½ìš° | ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿê°’ì´ ë²”ì£¼í˜• ë³€ìˆ˜ì¸ ê²½ìš° |
    | ì—°ì†ì  ì˜ˆì¸¡ê°’ | ì´ì‚°ì  ì˜ˆì¸¡ê°’ |
    | ex) ì†í•´ì•¡, ë§¤ì¶œëŸ‰, ê±°ë˜ëŸ‰, íŒŒì‚° í™•ë¥  ë“± ì˜ˆì¸¡ | ex) ì´ì§„ ë¶„ë¥˜ / ë‹¤ì¤‘ ë¶„ë¥˜ |
</aside>


ì‹¬ì¥ë§ˆë¹„ ë°ì´í„°ì˜ ê²½ìš°, ì •ë‹µ ê°’ì´ 0ê³¼ 1ë¡œ ì´ë¤„ì§„ ì´ì§„ë¶„ë¥˜ ë¬¸ì œ

### â†’ ë¶„ë¥˜ ëª¨ë¸ ì‚¬ìš©

## 1. ê²°ì • íŠ¸ë¦¬

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error

# í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ ì´ˆê¸°í™”
model = DecisionTreeClassifier(random_state=42)

# ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# ì •í™•ë„ ì¶œë ¥
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"F1 Score: {f1}")
```

```
Accuracy: 0.6956521739130435
F1 Score: 0.75
```

## 2. ëœë¤ í¬ë ˆìŠ¤íŠ¸

```python
from sklearn.ensemble import RandomForestClassifier

# í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì´ˆê¸°í™”
model = RandomForestClassifier(n_estimators=100, random_state=42)

# ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# ì •í™•ë„ ì¶œë ¥
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"F1 Score: {f1}")
```

```
Accuracy: 0.717391304347826
F1 Score: 0.7719298245614036
```

## 3. XGBoost

```python
import xgboost as xgb

# í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ
model = xgb.XGBClassifier(objective="multi:softmax", num_class=3, random_state=42)
model.fit(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
y_pred = model.predict(X_test)

# ì •í™•ë„ ì¶œë ¥
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"F1 Score: {f1}")
```

```
Accuracy: 0.8043478260869565
F1 Score: 0.8421052631578947
```

## 4. ë¡œì§€ìŠ¤í‹± íšŒê·€

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, classification_report

# ì¸ì½”ë”©ëœ ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•˜ì—¬ X, y ì •ì˜
X = df1.drop(['output'], axis=1) #output drop!
y = df1[['output']]

# ë°ì´í„°ë¥¼ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™” ë° ì—°ì†í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X_train[con_cols] = scaler.fit_transform(X_train[con_cols])
X_test[con_cols] = scaler.transform(X_test[con_cols])

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ
log_reg = LogisticRegression(max_iter=1000)  # ë°˜ë³µ íšŸìˆ˜ ì„¤ì •
log_reg.fit(X_train, y_train.values.ravel())  # ëª¨ë¸ í•™ìŠµ

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ì˜ˆì¸¡
y_pred = log_reg.predict(X_test)

# ì •í™•ë„ ê³„ì‚° ë° ì¶œë ¥
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# ë¶„ë¥˜ ë³´ê³ ì„œ ì¶œë ¥
print(classification_report(y_test, y_pred))
```

```
Accuracy: 0.90163934426229
F1 Score: 0.90
```

## í‰ê°€

|  | Accuracy | F1 Score |
| --- | --- | --- |
| 1. ê²°ì • íŠ¸ë¦¬ | 0.6956521739130435 | 0.75 |
| 2. ëœë¤ í¬ë ˆìŠ¤íŠ¸ | 0.717391304347826 | 0.7719298245614036 |
| 3. XGBoost | 0.8043478260869565 | 0.8421052631578947 |
| 4. ë¡œì§€ìŠ¤í‹± íšŒê·€ | 0.90163934426229 | 0.90 |

### â†’ ìµœì ì˜ ëª¨ë¸: ë¡œì§€ìŠ¤í‹± íšŒê·€
